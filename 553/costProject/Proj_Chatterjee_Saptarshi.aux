\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Configuration 1: Hadoop/Spark Cluster with 32K-cores, 256TB memory, 50PB HDD, and 10Gb/s Ethernet Fat-Tree network (each VM should be equivalent to the d2.8xlarge instance); in addition to the compute resources, a 100PB distributed storage shared across the entire cloud should be procured, with enough capacity for 100GB/sec throughput (for pricing comparison, see S3)}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Configuration 2: Support 1 million virtual machines (VM) where each VM requires 2-core, 15GB RAM, 32GB SSD storage, and 1Gb/s Fat-Tree network (each VM should be equivalent to the r3.large instances); in addition to the compute resources, a 10PB distributed storage shared across the entire cloud should be procured, with enough capacity for 10GB/sec throughput (for pricing comparison, see S3)}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Configuration 3: Support deep learning with 1 exaflop of mixed precision performance (hint: each VM should be equivalent to p3.16xlarge instances; you will want to use the NVIDIA V100 GPUs (8 GPUs per node), and allocate 8-cores per GPU (64-cores per node) with 8GB of memory per core (512GB per node); the network to use is at least 10Gb/s per GPU (100Gb/s should work), and should be organized in a Fat-Tree network; in addition to the compute resources, a 1PB distributed storage shared across the entire cloud should be procured, with enough capacity for 10GB/sec throughput (for pricing comparison, see S3)}{7}}
